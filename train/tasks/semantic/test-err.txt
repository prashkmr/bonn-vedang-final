2023-03-29 06:40:30.729206: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-29 06:40:36.134329: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-03-29 06:40:45.954774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/eqdong/dslr/lib/python3.8/site-packages/cv2/../../lib64:
2023-03-29 06:40:45.956707: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/eqdong/dslr/lib/python3.8/site-packages/cv2/../../lib64:
2023-03-29 06:40:45.956775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-03-29 06:41:04.388074: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-29 06:41:48.457375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29849 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0
/home/eqdong/dslr/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Traceback (most recent call last):
  File "train.py", line 122, in <module>
    trainer.train()
  File "/scratch/eqdong/diff-lidar-slam-dslr*/seg/bonnetal_vedang/train/tasks/semantic/../../tasks/semantic/modules/trainer.py", line 319, in train
    acc, iou, loss, update_mean = self.train_epoch(train_loader,
  File "/scratch/eqdong/diff-lidar-slam-dslr*/seg/bonnetal_vedang/train/tasks/semantic/../../tasks/semantic/modules/trainer.py", line 548, in train_epoch
    slam_err        = Slam_error(trues, preds)
  File "/scratch/eqdong/diff-lidar-slam-dslr*/seg/bonnetal_vedang/train/tasks/semantic/../../tasks/semantic/modules/SLAM_ERROR.py", line 97, in Slam_error
    gnd_graph=find_poseGraph(gradslam.Pointclouds(gnd_scans,gnd_scan_normals))
  File "/scratch/eqdong/diff-lidar-slam-dslr*/seg/bonnetal_vedang/train/tasks/semantic/../../tasks/semantic/modules/SLAM_ERROR.py", line 83, in find_poseGraph
    pcd_pose=compose_transformations(poses[-1].squeeze(1),transform.squeeze(1)).unsqueeze(1)
  File "/home/eqdong/dslr/lib/python3.8/site-packages/kornia/geometry/linalg.py", line 64, in compose_transformations
    rmat_02: torch.Tensor = torch.matmul(rmat_01, rmat_12)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_bmm)
